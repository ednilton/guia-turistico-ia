import os
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional
import uvicorn

# Carregar vari√°veis de ambiente
load_dotenv()

# Verificar configura√ß√£o na inicializa√ß√£o
print("üîß Inicializando aplica√ß√£o...")
openai_key = os.getenv("OPENAI_API_KEY")
print(f"üîç OpenAI key configured: {bool(openai_key)}")
if openai_key:
    print(f"üîç OpenAI key prefix: {openai_key[:10]}...")
else:
    print("‚ö†Ô∏è WARNING: OPENAI_API_KEY not found in environment")

# Importa√ß√µes do LangChain
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

# Criar inst√¢ncia do FastAPI
app = FastAPI(
    title="Chat Inteligente API",
    description="API para o projeto de chat inteligente com LangChain",
    version="1.0.0"
)

# Modelos Pydantic para as requisi√ß√µes
class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = "default_session"

class ChatResponse(BaseModel):
    response: str
    session_id: str
    model_used: str

# Configura√ß√£o do LangChain
template = """
Voc√™ √© um assistente de Viagem que ajuda os usu√°rios a planejar suas viagens, 
dando sugest√µes de destinos, roteiros e dicas pr√°ticas. A primeira coisa que 
voc√™ deve fazer √© perguntar ao usu√°rio qual √© o destino da viagem e com 
quantas pessoas ele est√° viajando.

Hist√≥rico da conversa: {history}

Entrada do usu√°rio: {input}
"""

prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}")
])

# Inicializar o modelo
try:
    llm = ChatOpenAI(
        temperature=0.7, 
        model="gpt-3.5-turbo",  # Modelo mais barato
        openai_api_key=os.getenv("OPENAI_API_KEY")
    )
    print("‚úÖ LLM initialized successfully")
except Exception as e:
    print(f"‚ùå Error initializing LLM: {e}")
    llm = None

# Criar a chain
chain = prompt | llm

# Store para hist√≥rico de conversas
store = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    """Obt√©m o hist√≥rico de mensagens para uma sess√£o espec√≠fica."""
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# Chain com hist√≥rico
chain_with_history = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history"
)

# Endpoints da API
@app.get("/")
async def root():
    """Endpoint raiz"""
    return {
        "message": "üöÄ Chat Inteligente API est√° funcionando!",
        "status": "success",
        "version": "1.0.0",
        "features": ["FastAPI", "LangChain", "OpenAI", "Chat History"]
    }

@app.get("/health")
async def health_check():
    """Verificar sa√∫de da API"""
    # Verificar se a chave da OpenAI est√° configurada
    openai_configured = bool(os.getenv("OPENAI_API_KEY"))
    
    return {
        "status": "healthy",
        "service": "chat-inteligente",
        "openai_configured": openai_configured,
        "langchain_ready": True
    }

@app.get("/info")
async def info():
    """Informa√ß√µes sobre a API"""
    return {
        "name": "Chat Inteligente",
        "description": "API para chat com IA usando LangChain",
        "model": "gpt-4o-mini",
        "features": [
            "Assistente de Viagem",
            "Hist√≥rico de Conversas",
            "M√∫ltiplas Sess√µes"
        ],
        "endpoints": {
            "root": "/",
            "health": "/health",
            "chat": "/chat",
            "sessions": "/sessions",
            "docs": "/docs",
            "redoc": "/redoc"
        }
    }

@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    """Endpoint principal de chat"""
    try:
        # Debug: Verificar vari√°veis de ambiente
        openai_key = os.getenv("OPENAI_API_KEY")
        print(f"üîç DEBUG: OpenAI key configured: {bool(openai_key)}")
        if openai_key:
            print(f"üîç DEBUG: OpenAI key starts with: {openai_key[:10]}...")
        
        # Verificar se a chave da OpenAI est√° configurada
        if not openai_key:
            print("‚ùå ERROR: OPENAI_API_KEY n√£o encontrada")
            raise HTTPException(
                status_code=500, 
                detail="OPENAI_API_KEY n√£o configurada. Adicione no arquivo .env"
            )
        
        print(f"üîç DEBUG: Processing message: {request.message[:50]}...")
        print(f"üîç DEBUG: Session ID: {request.session_id}")
        
        # Processar a mensagem
        resposta = chain_with_history.invoke(
            {'input': request.message},
            config={'configurable': {'session_id': request.session_id}}
        )
        
        print(f"‚úÖ DEBUG: Response generated successfully")
        
        return ChatResponse(
            response=resposta.content,
            session_id=request.session_id,
            model_used="gpt-3.5-turbo"
        )
        
    except Exception as e:
        print(f"‚ùå ERROR in chat endpoint: {str(e)}")
        print(f"‚ùå ERROR type: {type(e).__name__}")
        import traceback
        print(f"‚ùå TRACEBACK: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Erro no processamento: {str(e)}")

@app.get("/sessions")
async def list_sessions():
    """Listar sess√µes ativas"""
    return {
        "active_sessions": list(store.keys()),
        "total_sessions": len(store)
    }

@app.get("/sessions/{session_id}/history")
async def get_session_history_endpoint(session_id: str):
    """Obter hist√≥rico de uma sess√£o espec√≠fica"""
    if session_id not in store:
        raise HTTPException(status_code=404, detail="Sess√£o n√£o encontrada")
    
    history = store[session_id]
    messages = []
    
    for message in history.messages:
        messages.append({
            "type": message.__class__.__name__,
            "content": message.content
        })
    
    return {
        "session_id": session_id,
        "messages": messages,
        "total_messages": len(messages)
    }

@app.delete("/sessions/{session_id}")
async def clear_session(session_id: str):
    """Limpar hist√≥rico de uma sess√£o"""
    if session_id in store:
        del store[session_id]
        return {"message": f"Sess√£o {session_id} limpa com sucesso"}
    else:
        raise HTTPException(status_code=404, detail="Sess√£o n√£o encontrada")

@app.delete("/sessions")
async def clear_all_sessions():
    """Limpar todas as sess√µes"""
    store.clear()
    return {"message": "Todas as sess√µes foram limpas"}

# Fun√ß√£o para executar o servidor
def run_server():
    """Executar o servidor FastAPI"""
    uvicorn.run(
        "main:app",
        host="127.0.0.1",
        port=8000,
        reload=True,
        log_level="info"
    )

if __name__ == "__main__":
    run_server()